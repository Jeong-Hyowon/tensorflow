{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1., 0., 3., 0., 5.],[0., 2., 0., 4., 0.]]\n",
    "y_data = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.uniform((1,2), -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-7-97617f7d4374>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-97617f7d4374>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    print((\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))\u001b[0m\n\u001b[1;37m                                                                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W,b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print((\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  23.134184 |    -0.3966 |    -0.5744 |   0.119513\n",
      "   50 |   6.508003 |     0.2697 |    -0.0835 |   0.432979\n",
      "  100 |   2.081481 |     0.5703 |     0.2227 |   0.592919\n",
      "  150 |   0.781385 |     0.7041 |     0.4163 |   0.674131\n",
      "  200 |   0.356422 |     0.7630 |     0.5401 |   0.714079\n",
      "  250 |   0.203022 |     0.7885 |     0.6203 |   0.731887\n",
      "  300 |   0.142676 |     0.7998 |     0.6729 |   0.737534\n",
      "  350 |   0.116850 |     0.8052 |     0.7079 |   0.736357\n",
      "  400 |   0.104565 |     0.8083 |     0.7317 |   0.731283\n",
      "  450 |   0.097798 |     0.8106 |     0.7483 |   0.723961\n",
      "  500 |   0.093349 |     0.8129 |     0.7601 |   0.715342\n",
      "  550 |   0.089912 |     0.8151 |     0.7690 |   0.705988\n",
      "  600 |   0.086945 |     0.8175 |     0.7758 |   0.696237\n",
      "  650 |   0.084221 |     0.8200 |     0.7814 |   0.686294\n",
      "  700 |   0.081643 |     0.8225 |     0.7861 |   0.676286\n",
      "  750 |   0.079170 |     0.8250 |     0.7903 |   0.666293\n",
      "  800 |   0.076782 |     0.8276 |     0.7940 |   0.656363\n",
      "  850 |   0.074471 |     0.8301 |     0.7976 |   0.646527\n",
      "  900 |   0.072231 |     0.8327 |     0.8009 |   0.636803\n",
      "  950 |   0.070060 |     0.8352 |     0.8040 |   0.627204\n",
      " 1000 |   0.067954 |     0.8376 |     0.8071 |   0.617734\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], # bias(b)\n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.uniform((1,3), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   9.098433 |    -0.6040 |     0.4898 |    -0.0147\n",
      "   50 |   2.938319 |    -0.3977 |     0.8104 |     0.3691\n",
      "  100 |   1.041765 |    -0.2836 |     0.9492 |     0.6108\n",
      "  150 |   0.396617 |    -0.2184 |     1.0068 |     0.7640\n",
      "  200 |   0.158809 |    -0.1799 |     1.0288 |     0.8616\n",
      "  250 |   0.066160 |    -0.1565 |     1.0358 |     0.9239\n",
      "  300 |   0.028796 |    -0.1416 |     1.0369 |     0.9638\n",
      "  350 |   0.013408 |    -0.1318 |     1.0359 |     0.9893\n",
      "  400 |   0.006979 |    -0.1250 |     1.0344 |     1.0055\n",
      "  450 |   0.004254 |    -0.1201 |     1.0329 |     1.0158\n",
      "  500 |   0.003073 |    -0.1164 |     1.0316 |     1.0223\n",
      "  550 |   0.002539 |    -0.1135 |     1.0305 |     1.0263\n",
      "  600 |   0.002277 |    -0.1110 |     1.0297 |     1.0287\n",
      "  650 |   0.002131 |    -0.1088 |     1.0289 |     1.0301\n",
      "  700 |   0.002034 |    -0.1068 |     1.0283 |     1.0309\n",
      "  750 |   0.001960 |    -0.1050 |     1.0278 |     1.0312\n",
      "  800 |   0.001895 |    -0.1033 |     1.0273 |     1.0312\n",
      "  850 |   0.001835 |    -0.1016 |     1.0268 |     1.0310\n",
      "  900 |   0.001779 |    -0.1001 |     1.0263 |     1.0308\n",
      "  950 |   0.001725 |    -0.0985 |     1.0259 |     1.0304\n",
      " 1000 |   0.001673 |    -0.0970 |     1.0255 |     1.0301\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis-y_data))\n",
    "        \n",
    "    grads = tape.gradient(cost,[W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
